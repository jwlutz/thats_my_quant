# ADR-0004: LangChain Limited to Polish-Only Integration

## Status
**ACCEPTED** - 2025-01-XX

## Context

After completing the foundational analysis engine and report infrastructure, we needed to integrate LLM capabilities for narrative generation while maintaining our strict no-hallucination principle and local-first architecture.

## Decision Drivers

1. **Anti-Hallucination**: Zero tolerance for invented numbers, dates, or facts
2. **Deterministic Fallback**: System must work without LLM dependency
3. **Local-First**: No external telemetry or cloud dependencies
4. **User Control**: Clear on/off switch for AI features
5. **Maintainability**: Minimal complexity, focused scope

## Considered Options

### Option A: Full LangChain Agents with Tools and Memory
Build comprehensive LangChain agents with tool access and conversation memory.
**Pros**: Rich AI capabilities, sophisticated interactions
**Cons**: Complex, high hallucination risk, scope creep potential, external dependencies

### Option B: Direct Ollama Integration Only
Continue with existing direct Ollama REST API calls.
**Pros**: Simple, minimal dependencies, working solution
**Cons**: Manual prompt engineering, no structured output parsing, limited retry logic

### Option C: LangChain Limited to Polish-Only Chains ✅
Use LangChain LCEL chains exclusively for text polishing with structured parsers.
**Pros**: Better output parsing, retry logic, structured prompts, contained scope
**Cons**: Slightly more complex than direct API calls

## Decision

**We choose Option C: LangChain Limited to Polish-Only Chains**

### Implementation Architecture

```
┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐
│   Skeleton      │    │   LangChain      │    │   Audited       │
│   Builder       │───▶│   LCEL Chain     │───▶│   Output        │
│   (Deterministic)│    │   + Parser       │    │   (Validated)   │
└─────────────────┘    └──────────────────┘    └─────────────────┘
                                │
                                ▼
                       ┌──────────────────┐
                       │   Number/Date    │
                       │   Audit System   │
                       │   (Fallback)     │
                       └──────────────────┘
```

### Key Components

1. **Executive Summary Chain**: LCEL chain with structured parser (120-180 words)
2. **Risk Bullets Chain**: LCEL chain generating 3-5 risk bullets
3. **Structured Parsers**: Pydantic-based parsers with format validation
4. **Number/Date Audit**: Regex-based extraction and tolerance validation
5. **Automatic Fallback**: Returns skeleton on audit failures

### Strict Boundaries

**LangChain IS used for:**
- ✅ Text polishing (skeleton → readable narrative)
- ✅ Structured output parsing (word limits, bullet counts)
- ✅ Prompt template management
- ✅ Retry logic with fallback

**LangChain is NOT used for:**
- ❌ Agents with tool access
- ❌ Memory or conversation history
- ❌ Multi-step reasoning chains
- ❌ Data calculation or analysis
- ❌ External API integrations

### Safety Guarantees

1. **Deterministic Parameters**: Temperature=0, top_p=1, repeat_penalty=1
2. **Input Validation**: All data pre-validated before LLM processing
3. **Output Audit**: Zero-tolerance number/date validation with ±0.05% tolerance
4. **Automatic Fallback**: Skeleton returned on any audit failure
5. **User Control**: `--llm=off` default, explicit opt-in required

## Implementation Results

### Components Delivered
- **LC0**: Minimal dependencies (langchain-core, langchain-ollama) with telemetry disabled
- **LC1**: Executive summary and risk bullets chains with structured parsers
- **LC2**: Comprehensive number/date audit system with deterministic regex extraction
- **LC3**: Integrated audit with both chains (automatic fallback)
- **LC4**: CLI integration with `--llm=on|off` switch (default off)

### Test Coverage
- **65 total tests** (36 audit + 20 chains + 9 integration)
- **All edge cases covered**: negative zero, date normalization, tolerance handling
- **Real implementation testing**: No mocked LangChain components

### Performance Characteristics
- **Fast Audit**: Regex + numeric comparison, no NLP heuristics
- **Minimal Retries**: Max 1 retry, then fallback (no exponential backoff)
- **Deterministic Output**: Same input always produces same result
- **Graceful Degradation**: Works perfectly with `--llm=off`

## Consequences

### Positive
- **Zero Hallucination Risk**: Bulletproof audit system prevents data invention
- **Maintainable**: Clear boundaries, focused scope, minimal complexity
- **User Controlled**: Explicit opt-in for AI features, deterministic by default
- **Local-First**: No telemetry, no external dependencies
- **Production Ready**: Comprehensive testing, robust error handling

### Negative
- **Limited AI Capabilities**: No advanced reasoning or multi-step workflows
- **Additional Complexity**: More code than direct API calls
- **Dependency Addition**: New packages (langchain-core, langchain-ollama)

### Risk Mitigation
- **Scope Discipline**: Explicit boundaries documented and enforced
- **Fallback Strategy**: System works perfectly without LLM
- **Audit System**: Mathematical validation prevents hallucinations
- **User Education**: Clear documentation on when to use `--llm=on`

## Validation Criteria

The implementation meets all acceptance criteria:

1. ✅ **One LLM call per section** (no multi-step chains)
2. ✅ **No post-hoc rewriting** (single polish pass)
3. ✅ **No streaming/agents/memory** (pure LCEL chains)
4. ✅ **Temperature 0, explicit options** (deterministic parameters)
5. ✅ **Single retry, then fallback** (no exponential backoff)
6. ✅ **Audit compares numbers/dates only** (no NLP heuristics)
7. ✅ **CLI --llm=off remains default** (explicit opt-in)

## Future Considerations

This architecture provides a solid foundation for potential future enhancements:

- **Sentiment Analysis**: Can use same audit approach for news-based insights
- **Multi-Ticker Reports**: Structured output parsing scales to portfolio analysis
- **Custom Risk Models**: Risk bullets chain can be enhanced with sector-specific templates

However, any future additions must maintain the same strict boundaries and safety guarantees established in this ADR.

---

**Decided by**: User + AI Architecture Review  
**Date**: 2025-01-XX  
**Implementation**: LC0-LC4 complete with comprehensive testing  
**Status**: Production ready, `--llm=off` by default
